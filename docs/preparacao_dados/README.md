# [üîô](../../README.md) Prepara√ß√£o de Dados

## Categoral Encoding

Transformar categorias em n√∫meros.

### Label Encoding

Usada quando h√° ordem, grande n√∫mero de categorias (n√£o d√° para usar One-hot Encoding).

Cada categoria recebe um n√∫mero, ou seja, a primeira categoria recebe 0, a segunda 1 e assim por diante. Geralmente em ordem alfabetica.

![alt text](image.png)

Pontos Fraco:
- Problema de interpreta√ß√£o de ordem de grandeza. Pequeno, M√©dio e Grande tem ordem de grandeza, mas cores vermelho, verde e azul n√£o tem. Pode gerar problema para o classificador.

Pontos Forte:
- Faixa de valores finitas.
- Faixa de valores discretas.

### One-Hot Encoding

Usada o n√∫mero de categorias √© pequeno e n√£o h√° ordem.

Cada categoria √© transformada em outro atributo: **Dummy variable**.

Um valor bin√°rio informa a ocorr√™ncia.

![alt text](image-1.png)

Pontos Fraco:
- Muitas colunas podem gerar um espa√ßo de caracter√≠sticas muito grande (alta dimens√£o). Pode causar um super ajuste e ter alto custo computacional.
- Maldi√ß√£o da dimensionalidade: Dados espar√ßos, muitas colunas com valore zero. Dificil encontrar valor de interesse.
- Dummy variable Trap: Valores de colunas bin√°rias podem ser previstos a partr dos valores de outras colunas. Tipo, d√° para deduzir os valores onde tem interroga√ß√£o na imagem.

    ![alt text](image-2.png)
    - Valor de atributos altamente previs√≠veis.
    - Multicolinearidade: corre√ß√£o entre as vari√°veis independentes
    - A solu√ß√£o √© excluir um dos atributos ou combinar colunas bin√°rias.
    ![alt text](image-3.png)


## Dimensionamento de Caracter√≠sticas (Feature Scaling)

Processo de tranforma√ß√£o de dados num√©ricos. 

Observe no exemplo que cada atributo esta em uma escala (unidade, dezena, centena, etc.). Elas v√£o contribuir de forma desbalanceada no modelo.
![alt text](image-4.png)

Permite no Gradient Descent convergencia mais r√°pida para ao m√≠nimo local.

Veja a compara√ß√£o dos metodos no cl√°ssico problema da IRIS.
![alt text](image-5.png)


Observa√ß√µes:
- N√£o vai nencessariamente melhorar o modelo, dependendo do problema;
- Arvores de decis√£o n√£o de precisam de nenhum tipo de dimensionamento de caracter√≠sticas;
- N√£o se aplica a atributos categ√≥ricos j√° transformados (ex. one-hot encoding).

### Padroniza√ß√£o (Z-Score)

Normaliza os dados para ter media 0 e variancia 1.

$$X_p = \frac{X - \mu}{\sigma}$$

- Dados aproximado da media zero e desvio padr√£o 1 (pr√≥ximos de uma distribui√ß√£o normal);
- Podem ser negativos;
- N√£o afeta outliers;
- Deve ser usado na maioria dos casos.



### Normaliza√ß√£o (Min-Max)

Normaliza os dados para ter min 0 e max 1.

$$X_p = \frac{X - min(X)}{max(X) - min(X)}$$

- Transforma para escala comum entre zero e 1;
- usado em processamento de imagens e RNA;
- Quando n√£o sabemos a distribui√ß√£o dos dados;
- Quando precisam ser positivos;
- Algor√≠timos n√£o "requerem" dados normais;
- Remove outliers pois imp√µes "limites".


## Normalization